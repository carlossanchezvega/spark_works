{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARLOS S√ÅNCHEZ VEGA\n",
    "\n",
    "# STRUCTURED STREAMING EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Get the urls the most common in  a log file</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import col, asc,desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the spark configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"StructuredStreaming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.textFile(\"access_log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.249.75.159 - - [29/Nov/2015:03:50:05 +0000] \"GET /robots.txt HTTP/1.1\" 200 55 \"-\" \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n",
      "66.249.75.168 - - [29/Nov/2015:03:50:06 +0000] \"GET /blog/ HTTP/1.1\" 200 8083 \"-\" \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n",
      "185.71.216.232 - - [29/Nov/2015:03:53:15 +0000] \"POST /wp-login.php HTTP/1.1\" 200 1691 \"http://nohatenews.com/wp-login.php\" \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0\"\n",
      "54.165.199.171 - - [29/Nov/2015:04:32:27 +0000] \"GET /sitemap_index.xml HTTP/1.0\" 200 592 \"-\" \"W3 Total Cache/0.9.4.1\"\n",
      "54.165.199.171 - - [29/Nov/2015:04:32:27 +0000] \"GET /post-sitemap.xml HTTP/1.0\" 200 2502 \"-\" \"W3 Total Cache/0.9.4.1\"\n",
      "54.165.199.171 - - [29/Nov/2015:04:32:27 +0000] \"GET /page-sitemap.xml HTTP/1.0\" 200 11462 \"-\" \"W3 Total Cache/0.9.4.1\"\n",
      "54.165.199.171 - - [29/Nov/2015:04:32:27 +0000] \"GET /category-sitemap.xml HTTP/1.0\" 200 585 \"-\" \"W3 Total Cache/0.9.4.1\"\n",
      "54.165.199.171 - - [29/Nov/2015:04:32:27 +0000] \"GET /blog/ HTTP/1.0\" 200 31746 \"-\" \"-\"\n",
      "54.165.199.171 - - [29/Nov/2015:04:32:27 +0000] \"GET /orlando-sports/ HTTP/1.0\" 200 35510 \"-\" \"-\"\n",
      "54.165.199.171 - - [29/Nov/2015:04:32:37 +0000] \"GET /about/ HTTP/1.0\" 200 25121 \"-\" \"-\"\n"
     ]
    }
   ],
   "source": [
    "for i in lines.take(10):print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse out the common log format to a DataFrame\n",
    "contentSizeExp = r'\\s(\\d+)$'\n",
    "statusExp = r'\\s(\\d{3})\\s'\n",
    "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "timeExp = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "hostExp = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to analyse all files created in the logs diretory, in such a way that every time a file is added to that folder, the analysis updates its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessLines = spark.readStream.text(\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse data entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsDF = accessLines.select(func.regexp_extract('value', hostExp, 1).alias('host'),\n",
    "                         func.regexp_extract('value', timeExp, 1).alias('timestamp'),\n",
    "                         func.regexp_extract('value', generalExp, 1).alias('method'),\n",
    "                         func.regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
    "                         func.regexp_extract('value', generalExp, 3).alias('protocol'),\n",
    "                         func.regexp_extract('value', statusExp, 1).cast('integer').alias('status'),\n",
    "                         func.regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsDF2 = logsDF.withColumn(\"eventTime\", func.current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a window of 30 seconds and, every 10 seconds the analysis get re-calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpointCounts = logsDF2.groupBy(func.window(func.col(\"eventTime\"), \\\n",
    "      \"30 seconds\", \"10 seconds\"), func.col(\"endpoint\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedEndpointCounts = endpointCounts.orderBy(func.col(\"count\").desc())\n",
    "\n",
    "# we kick off our streaming query and display the stream to the console\n",
    "query = sortedEndpointCounts.writeStream.outputMode(\"complete\").format(\"console\") \\\n",
    "      .queryName(\"counts\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until we terminate the scripts\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we insert the same file two times to the \"log\" fokder, we will have the nex output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+------+-----+\n",
    "|status|count|\n",
    "+------+-----+\n",
    "|   500|10714|\n",
    "|   301|  271|\n",
    "|   400|    2|\n",
    "|   404|   26|\n",
    "|   200|64971|\n",
    "|   304|   92|\n",
    "|   302|    2|\n",
    "|   405|    1|\n",
    "+------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+------+------+\n",
    "|status| count|\n",
    "+------+------+\n",
    "|   500| 21428|\n",
    "|   301|   542|\n",
    "|   400|     4|\n",
    "|   404|    52|\n",
    "|   200|129942|\n",
    "|   304|   184|\n",
    "|   302|     4|\n",
    "|   405|     2|\n",
    "+------+------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
